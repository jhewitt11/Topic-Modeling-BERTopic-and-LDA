
The results below are from a small sample, each model was run 8 times with the parameters below. 

~~~
BERTopic:
	# default parameters
	model_name = "all-MiniLM-L6-v2"
	min_topic_size = 10

LDA:
	CV_grid = {
		'max_df': 0.85, 
		'min_df': 0.1, 
		'ngram_range': (1, 1), 
	}
	LDA_grid = {    
		'LDA__n_components': 15, 
		'LDA__doc_topic_prior': 0.5, 
		'LDA__topic_word_prior': 0.5,
		'LDA__max_iter' : 100,
	}
	
General parameters:
	iters = 8
	N = 10
~~~

BERTopic consistently builds topics that hold documents from a single category with only a few outliers while the topics generated by LDA are messier. This makes sense and can be explained by BERTopic’s more sophisticated, semantically rich document vectors as well as it’s outlier topic which holds hard to place documents.

BERTopic’s Topic Coherence scores were also consistently higher than the LDA models. N defines how many of the most frequently occuring words in a topic are used to calculate a topic’s NPMI score. This score is not weighted by the number of documents in a topic, and TC is the average of NPMI scores. 

# table max/min/average/std dev

Large topics, no matter the category consistently had a low NPMI score. LDA’s highest scoring topics were around 0.4 – 0.5. BERTopic regularly made topics that scored above 0.5, with a few topics scoring the maximum, 1.
